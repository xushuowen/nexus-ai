# Project Nexus Configuration
app:
  name: "Nexus AI"
  version: "0.1.0"
  host: "0.0.0.0"
  port: 8000
  debug: false

budget:
  daily_limit_tokens: 50000
  per_request_max_tokens: 2000
  curiosity_daily_ops: 10
  curiosity_per_op_tokens: 500
  warning_threshold: 0.8
  hard_stop: true
  reset_hour: 0

memory:
  working_memory_slots: 7
  episodic_max_entries: 1000
  semantic_decay_rate: 0.01
  hebbian_learning_rate: 0.1
  consolidation_interval_minutes: 30
  vector_store_path: "./data/chroma"
  sqlite_path: "./data/nexus.db"

orchestrator:
  max_parallel_hypotheses: 3
  confidence_threshold: 0.7
  auto_prune_below: 0.3
  simple_question_threshold: 0.85

providers:
  primary: "gemini-flash"
  fallback: "groq-llama"
  models:
    gemini-flash:
      model_id: "gemini/gemini-2.0-flash"
      max_tokens: 4096
      temperature: 0.7
      use_for: ["general", "routing", "simple_tasks", "classification", "analysis"]
    gemini-pro:
      model_id: "gemini/gemini-2.0-flash"
      max_tokens: 8192
      temperature: 0.7
      use_for: ["complex_reasoning", "code_generation"]
    gemini-vision:
      model_id: "gemini/gemini-2.0-flash"
      max_tokens: 4096
      temperature: 0.7
      use_for: ["vision"]
    github-gpt4o-mini:
      model_id: "openai/gpt-4o-mini"
      api_base: "https://models.inference.ai.azure.com"
      max_tokens: 4096
      temperature: 0.7
      use_for: []
    groq-llama:
      model_id: "groq/llama-3.3-70b-versatile"
      max_tokens: 2000
      temperature: 0.7
      use_for: ["fallback"]

security:
  sandbox_enabled: true
  allowed_paths:
    - "./data"
    - "./workspace"
  blocked_commands:
    - "rm -rf /"
    - "format"
    - "del /f /s /q"
  rate_limit_per_minute: 30

gateway:
  telegram:
    enabled: true
    token: ""
  web:
    enabled: true
  api:
    enabled: true

logging:
  level: "INFO"
  file: "./data/nexus.log"
