"""News aggregation skill using RSS feeds (free, no API key)."""

from __future__ import annotations

import re
from typing import Any

from nexus.skills.skill_base import BaseSkill, SkillResult


# Category â†’ RSS feed URLs
RSS_FEEDS = {
    "ç§‘æŠ€": [
        ("TechCrunch", "https://techcrunch.com/feed/"),
        ("The Verge", "https://www.theverge.com/rss/index.xml"),
        ("Ars Technica", "https://feeds.arstechnica.com/arstechnica/index"),
    ],
    "ä¸–ç•Œ": [
        ("BBC World", "https://feeds.bbci.co.uk/news/world/rss.xml"),
        ("Reuters", "https://www.reutersagency.com/feed/"),
    ],
    "å¥åº·": [
        ("Medical News Today", "https://www.medicalnewstoday.com/newsfeeds/rss"),
        ("WHO News", "https://www.who.int/rss-feeds/news-english.xml"),
    ],
    "å°ç£": [
        ("ä¸­å¤®ç¤¾", "https://feeds.feedburner.com/rsscna/politics"),
    ],
}

CATEGORY_ALIASES = {
    "tech": "ç§‘æŠ€", "technology": "ç§‘æŠ€", "ç§‘æŠ€": "ç§‘æŠ€",
    "world": "ä¸–ç•Œ", "åœ‹éš›": "ä¸–ç•Œ", "ä¸–ç•Œ": "ä¸–ç•Œ",
    "health": "å¥åº·", "é†«ç™‚": "å¥åº·", "å¥åº·": "å¥åº·",
    "taiwan": "å°ç£", "tw": "å°ç£", "å°ç£": "å°ç£",
}


class NewsSkill(BaseSkill):
    name = "news"
    description = "æ–°èžèšåˆ â€” å¾ž RSS æºå–å¾—æœ€æ–°æ–°èžï¼ˆç§‘æŠ€ã€ä¸–ç•Œã€å¥åº·ã€å°ç£ï¼‰"
    triggers = ["æ–°èž", "news", "é ­æ¢", "headline", "æœ€æ–°æ¶ˆæ¯", "headlines"]
    intent_patterns = [
        r"(æœ€è¿‘|é€™é€±|æœ€æ–°).{0,10}(ç™¼ç”Ÿ|æ¶ˆæ¯|å¤§äº‹|æ™‚äº‹|å‹•æ…‹)",
        r"æœ‰ä»€éº¼.{0,10}(å¤§äº‹|æ–°é®®äº‹|å€¼å¾—é—œæ³¨|æ–°èž|é‡è¦)",
        r"(ä¸–ç•Œ|å°ç£|åœ‹éš›|ç§‘æŠ€|å¥åº·|é†«ç™‚).{0,5}(æœ€æ–°|ç™¼ç”Ÿäº†ä»€éº¼|æ–°èž|å‹•æ…‹)",
        r"å¹«æˆ‘(çœ‹|æ‰¾|æŸ¥).{0,5}(æ–°èž|æ¶ˆæ¯|æ™‚äº‹|é ­æ¢)",
        r"(ä»Šå¤©|æœ€è¿‘).{0,5}(æœ‰ä»€éº¼|ä»€éº¼).{0,5}(æ–°èž|å¤§äº‹|æ¶ˆæ¯|é ­æ¢)",
        r"(é—œå¿ƒ|äº†è§£|çŸ¥é“).{0,10}(æœ€æ–°|ä»Šæ—¥|è¿‘æœŸ).{0,5}(å‹•æ…‹|æ¶ˆæ¯|æ™‚äº‹)",
    ]
    category = "information"
    requires_llm = False

    instructions = (
        "æ–°èžèšåˆï¼ˆRSSï¼‰ï¼š\n"
        "1. æ‰€æœ‰æ–°èžï¼šã€Œæ–°èžã€\n"
        "2. åˆ†é¡žï¼šã€Œæ–°èž ç§‘æŠ€ã€ã€Œnews healthã€\n"
        "3. æ”¯æ´åˆ†é¡žï¼šç§‘æŠ€ã€ä¸–ç•Œã€å¥åº·ã€å°ç£"
    )

    async def execute(self, query: str, context: dict[str, Any]) -> SkillResult:
        # Detect category
        category = self._detect_category(query)
        feeds = RSS_FEEDS.get(category, []) if category else []

        if not feeds:
            # Fetch from all categories
            all_feeds = []
            for cat_feeds in RSS_FEEDS.values():
                all_feeds.extend(cat_feeds)
            feeds = all_feeds
            category = "æ‰€æœ‰"

        try:
            articles = await self._fetch_feeds(feeds)
            if not articles:
                return SkillResult(content=f"ç„¡æ³•å–å¾—{category}æ–°èžã€‚", success=False, source=self.name)

            lines = [f"ðŸ“° **{category}æ–°èž**ï¼ˆ{len(articles)} å‰‡ï¼‰\n"]
            for i, (source, title, link, pub_date) in enumerate(articles[:8], 1):
                lines.append(f"**{i}. {title}**")
                lines.append(f"   ðŸ“Œ {source}" + (f" Â· {pub_date}" if pub_date else ""))
                if link:
                    lines.append(f"   ðŸ”— {link}")
                lines.append("")

            return SkillResult(content="\n".join(lines), success=True, source=self.name)

        except Exception as e:
            return SkillResult(content=f"æ–°èžå–å¾—å¤±æ•—: {e}", success=False, source=self.name)

    def _detect_category(self, query: str) -> str | None:
        text = query.lower()
        for alias, cat in CATEGORY_ALIASES.items():
            if alias in text:
                return cat
        return None

    async def _fetch_feeds(self, feeds: list[tuple[str, str]]) -> list[tuple[str, str, str, str]]:
        """Fetch and parse RSS feeds. Returns [(source, title, link, date)]."""
        articles = []

        try:
            import feedparser
        except ImportError:
            # Fallback to httpx + basic XML parsing
            return await self._fetch_feeds_basic(feeds)

        import httpx
        async with httpx.AsyncClient(timeout=10, follow_redirects=True) as client:
            for source_name, url in feeds:
                try:
                    resp = await client.get(url, headers={"User-Agent": "Nexus-AI/1.0"})
                    feed = feedparser.parse(resp.text)
                    for entry in feed.entries[:5]:
                        title = entry.get("title", "").strip()
                        link = entry.get("link", "")
                        pub = entry.get("published", "")[:16] if entry.get("published") else ""
                        if title:
                            articles.append((source_name, title, link, pub))
                except Exception:
                    continue

        return articles

    async def _fetch_feeds_basic(self, feeds: list[tuple[str, str]]) -> list[tuple[str, str, str, str]]:
        """Basic XML RSS parsing without feedparser."""
        articles = []
        import httpx

        async with httpx.AsyncClient(timeout=10, follow_redirects=True) as client:
            for source_name, url in feeds:
                try:
                    resp = await client.get(url, headers={"User-Agent": "Nexus-AI/1.0"})
                    text = resp.text
                    titles = re.findall(r'<title[^>]*>(.*?)</title>', text, re.DOTALL)
                    links = re.findall(r'<link[^>]*>(.*?)</link>', text, re.DOTALL)
                    for i, title in enumerate(titles[1:6]):  # skip feed title
                        title = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', title).strip()
                        title = re.sub(r'<[^>]+>', '', title)
                        link = links[i + 1].strip() if i + 1 < len(links) else ""
                        if title:
                            articles.append((source_name, title, link, ""))
                except Exception:
                    continue

        return articles
