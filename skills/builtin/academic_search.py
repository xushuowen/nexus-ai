"""Academic paper search skill - PubMed + Semantic Scholar + OpenAlex (all free)."""

from __future__ import annotations

import re
import xml.etree.ElementTree as ET
from typing import Any

from nexus.skills.skill_base import BaseSkill, SkillResult


# PT-related MeSH terms for auto-enhancement
PT_MESH_TERMS = {
    "ç‰©ç†æ²»ç™‚": "Physical Therapy Modalities[MeSH]",
    "physical therapy": "Physical Therapy Modalities[MeSH]",
    "å¾©å¥": "Rehabilitation[MeSH]",
    "rehabilitation": "Rehabilitation[MeSH]",
    "é‹å‹•æ²»ç™‚": "Exercise Therapy[MeSH]",
    "exercise therapy": "Exercise Therapy[MeSH]",
    "å¾’æ‰‹æ²»ç™‚": "Musculoskeletal Manipulations[MeSH]",
    "manual therapy": "Musculoskeletal Manipulations[MeSH]",
    "é›»ç™‚": "Electric Stimulation Therapy[MeSH]",
    "è¶…éŸ³æ³¢": "Ultrasonic Therapy[MeSH]",
    "ä¸­é¢¨": "Stroke[MeSH]",
    "stroke": "Stroke[MeSH]",
    "éª¨ç§‘": "Orthopedics[MeSH]",
    "è†é—œç¯€": "Knee Joint[MeSH]",
    "è‚©é—œç¯€": "Shoulder Joint[MeSH]",
    "è…°ç—›": "Low Back Pain[MeSH]",
    "low back pain": "Low Back Pain[MeSH]",
}


class AcademicSearchSkill(BaseSkill):
    name = "academic_search"
    description = "å­¸è¡“è«–æ–‡æœå°‹ â€” PubMedã€Semantic Scholarã€OpenAlexï¼ˆå…è²»ï¼Œç‰©ç†æ²»ç™‚å°ˆç”¨ï¼‰"
    triggers = [
        "è«–æ–‡", "paper", "æœŸåˆŠ", "journal", "pubmed", "ç ”ç©¶", "æ–‡ç»",
        "å­¸è¡“", "academic", "physical therapy", "ç‰©ç†æ²»ç™‚", "æ–‡ç»æœå°‹",
        "semantic scholar", "openalex",
    ]
    category = "academic"
    requires_llm = False

    instructions = (
        "å­¸è¡“æœå°‹ï¼š\n"
        "1. PubMedï¼šã€Œè«–æ–‡ physical therapy strokeã€\n"
        "2. Semantic Scholarï¼šã€Œè«–æ–‡ semantic scholar knee rehabilitationã€\n"
        "3. è‡ªå‹•å¢žå¼· PT ç›¸é—œ MeSH è¡“èªž"
    )

    async def execute(self, query: str, context: dict[str, Any]) -> SkillResult:
        # Clean query
        for t in self.triggers:
            query = query.replace(t, "").strip()
        query = query.strip()

        if not query or len(query) < 2:
            return SkillResult(
                content="è«‹æä¾›æœå°‹é—œéµå­—ï¼Œä¾‹å¦‚ï¼šã€Œè«–æ–‡ physical therapy stroke rehabilitationã€",
                success=False, source=self.name,
            )

        text_lower = query.lower()

        # Decide which database to search
        if "semantic scholar" in text_lower or "s2" in text_lower:
            query = query.replace("semantic scholar", "").replace("s2", "").strip()
            return await self._search_semantic_scholar(query)
        elif "openalex" in text_lower:
            query = query.replace("openalex", "").strip()
            return await self._search_openalex(query)
        else:
            # Default: PubMed (best for PT/medical)
            return await self._search_pubmed(query)

    async def _search_pubmed(self, query: str) -> SkillResult:
        """Search PubMed via E-utilities API (free, 3 req/sec)."""
        import httpx

        # Auto-enhance with MeSH terms
        enhanced_query = self._enhance_query(query)

        try:
            async with httpx.AsyncClient(timeout=15) as client:
                # Step 1: Search for PMIDs
                search_resp = await client.get(
                    "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
                    params={
                        "db": "pubmed",
                        "term": enhanced_query,
                        "retmode": "json",
                        "retmax": 8,
                        "sort": "relevance",
                    },
                )
                search_data = search_resp.json()
                pmids = search_data.get("esearchresult", {}).get("idlist", [])
                total = search_data.get("esearchresult", {}).get("count", "0")

                if not pmids:
                    return SkillResult(
                        content=f"PubMed æœå°‹ã€Œ{query}ã€æ²’æœ‰æ‰¾åˆ°çµæžœã€‚\næœå°‹èªžæ³•: {enhanced_query}",
                        success=True, source=self.name,
                    )

                # Step 2: Fetch article details
                fetch_resp = await client.get(
                    "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi",
                    params={
                        "db": "pubmed",
                        "id": ",".join(pmids),
                        "retmode": "xml",
                        "rettype": "abstract",
                    },
                )

                articles = self._parse_pubmed_xml(fetch_resp.text)

                lines = [f"ðŸ“š **PubMed æœå°‹çµæžœ**ï¼ˆå…± {total} ç­†ï¼Œé¡¯ç¤º {len(articles)} ç­†ï¼‰\n"]
                for i, article in enumerate(articles, 1):
                    lines.append(f"**{i}. {article['title']}**")
                    if article.get("authors"):
                        lines.append(f"   ðŸ‘¤ {article['authors']}")
                    if article.get("journal"):
                        lines.append(f"   ðŸ“– {article['journal']} ({article.get('year', '')})")
                    if article.get("pmid"):
                        lines.append(f"   ðŸ”— https://pubmed.ncbi.nlm.nih.gov/{article['pmid']}/")
                    if article.get("abstract"):
                        lines.append(f"   ðŸ“ {article['abstract'][:150]}...")
                    lines.append("")

                return SkillResult(content="\n".join(lines), success=True, source=self.name)

        except Exception as e:
            return SkillResult(content=f"PubMed æœå°‹å¤±æ•—: {e}", success=False, source=self.name)

    async def _search_semantic_scholar(self, query: str) -> SkillResult:
        """Search Semantic Scholar API (free, no key needed)."""
        import httpx

        try:
            async with httpx.AsyncClient(timeout=15) as client:
                resp = await client.get(
                    "https://api.semanticscholar.org/graph/v1/paper/search",
                    params={
                        "query": query,
                        "limit": 8,
                        "fields": "title,authors,year,abstract,url,citationCount,openAccessPdf",
                    },
                )
                resp.raise_for_status()
                data = resp.json()

            papers = data.get("data", [])
            total = data.get("total", 0)

            if not papers:
                return SkillResult(
                    content=f"Semantic Scholar æœå°‹ã€Œ{query}ã€æ²’æœ‰æ‰¾åˆ°çµæžœã€‚",
                    success=True, source=self.name,
                )

            lines = [f"ðŸ“š **Semantic Scholar æœå°‹çµæžœ**ï¼ˆå…± {total:,} ç­†ï¼‰\n"]
            for i, paper in enumerate(papers, 1):
                title = paper.get("title", "Untitled")
                year = paper.get("year", "")
                citations = paper.get("citationCount", 0)
                authors = ", ".join(a.get("name", "") for a in paper.get("authors", [])[:3])
                pdf = paper.get("openAccessPdf", {})
                pdf_url = pdf.get("url", "") if pdf else ""

                lines.append(f"**{i}. {title}**")
                if authors:
                    lines.append(f"   ðŸ‘¤ {authors}")
                lines.append(f"   ðŸ“… {year} | ðŸ“Š å¼•ç”¨: {citations}")
                if paper.get("url"):
                    lines.append(f"   ðŸ”— {paper['url']}")
                if pdf_url:
                    lines.append(f"   ðŸ“„ PDF: {pdf_url}")
                lines.append("")

            return SkillResult(content="\n".join(lines), success=True, source=self.name)

        except Exception as e:
            return SkillResult(content=f"Semantic Scholar æœå°‹å¤±æ•—: {e}", success=False, source=self.name)

    async def _search_openalex(self, query: str) -> SkillResult:
        """Search OpenAlex API (free, massive coverage)."""
        import httpx

        try:
            async with httpx.AsyncClient(timeout=15) as client:
                resp = await client.get(
                    "https://api.openalex.org/works",
                    params={"search": query, "per_page": 8},
                )
                resp.raise_for_status()
                data = resp.json()

            works = data.get("results", [])
            total = data.get("meta", {}).get("count", 0)

            if not works:
                return SkillResult(
                    content=f"OpenAlex æœå°‹ã€Œ{query}ã€æ²’æœ‰æ‰¾åˆ°çµæžœã€‚",
                    success=True, source=self.name,
                )

            lines = [f"ðŸ“š **OpenAlex æœå°‹çµæžœ**ï¼ˆå…± {total:,} ç­†ï¼‰\n"]
            for i, work in enumerate(works, 1):
                title = work.get("title", "Untitled")
                year = work.get("publication_year", "")
                cited = work.get("cited_by_count", 0)
                doi = work.get("doi", "")
                oa = work.get("open_access", {})
                oa_url = oa.get("oa_url", "") if oa else ""
                is_oa = oa.get("is_oa", False) if oa else False

                lines.append(f"**{i}. {title}**")
                lines.append(f"   ðŸ“… {year} | ðŸ“Š å¼•ç”¨: {cited}" + (" | ðŸ”“ Open Access" if is_oa else ""))
                if doi:
                    lines.append(f"   ðŸ”— {doi}")
                if oa_url:
                    lines.append(f"   ðŸ“„ PDF: {oa_url}")
                lines.append("")

            return SkillResult(content="\n".join(lines), success=True, source=self.name)

        except Exception as e:
            return SkillResult(content=f"OpenAlex æœå°‹å¤±æ•—: {e}", success=False, source=self.name)

    def _enhance_query(self, query: str) -> str:
        """Auto-enhance query with MeSH terms for PT-related searches."""
        query_lower = query.lower()
        for keyword, mesh in PT_MESH_TERMS.items():
            if keyword in query_lower:
                # Replace keyword with MeSH term for better PubMed results
                query = query_lower.replace(keyword, mesh, 1)
                return query
        return query

    def _parse_pubmed_xml(self, xml_text: str) -> list[dict[str, str]]:
        """Parse PubMed XML response into article dicts."""
        articles = []
        try:
            root = ET.fromstring(xml_text)
            for article_el in root.findall(".//PubmedArticle"):
                article = {}

                # PMID
                pmid_el = article_el.find(".//PMID")
                if pmid_el is not None:
                    article["pmid"] = pmid_el.text

                # Title
                title_el = article_el.find(".//ArticleTitle")
                if title_el is not None:
                    article["title"] = "".join(title_el.itertext()).strip()

                # Authors
                authors = []
                for author_el in article_el.findall(".//Author")[:3]:
                    last = author_el.findtext("LastName", "")
                    init = author_el.findtext("Initials", "")
                    if last:
                        authors.append(f"{last} {init}".strip())
                article["authors"] = ", ".join(authors) + (" et al." if len(article_el.findall(".//Author")) > 3 else "")

                # Journal
                journal_el = article_el.find(".//Journal/Title")
                if journal_el is not None:
                    article["journal"] = journal_el.text

                # Year
                year_el = article_el.find(".//PubDate/Year")
                if year_el is not None:
                    article["year"] = year_el.text

                # Abstract
                abstract_parts = []
                for abs_el in article_el.findall(".//AbstractText"):
                    text = "".join(abs_el.itertext()).strip()
                    if text:
                        abstract_parts.append(text)
                article["abstract"] = " ".join(abstract_parts)

                if article.get("title"):
                    articles.append(article)
        except ET.ParseError:
            pass
        return articles
