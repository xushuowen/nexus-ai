"""Academic paper search skill - PubMed + Semantic Scholar + OpenAlex (all free)."""

from __future__ import annotations

import re
import xml.etree.ElementTree as ET
from typing import Any

from nexus.skills.skill_base import BaseSkill, SkillResult


# PT-related MeSH terms for auto-enhancement
PT_MESH_TERMS = {
    "Áâ©ÁêÜÊ≤ªÁôÇ": "Physical Therapy Modalities[MeSH]",
    "physical therapy": "Physical Therapy Modalities[MeSH]",
    "physiotherapy": "Physical Therapy Modalities[MeSH]",
    "Âæ©ÂÅ•": "Rehabilitation[MeSH]",
    "rehabilitation": "Rehabilitation[MeSH]",
    "ÈÅãÂãïÊ≤ªÁôÇ": "Exercise Therapy[MeSH]",
    "exercise therapy": "Exercise Therapy[MeSH]",
    "ÂæíÊâãÊ≤ªÁôÇ": "Musculoskeletal Manipulations[MeSH]",
    "manual therapy": "Musculoskeletal Manipulations[MeSH]",
    "ÈõªÁôÇ": "Electric Stimulation Therapy[MeSH]",
    "Ë∂ÖÈü≥Ê≥¢": "Ultrasonic Therapy[MeSH]",
    "‰∏≠È¢®": "Stroke[MeSH]",
    "stroke": "Stroke[MeSH]",
    "È™®Áßë": "Orthopedics[MeSH]",
    # Knee
    "ËÜùÈóúÁØÄ": "Knee Joint[MeSH]",
    "ÂâçÂçÅÂ≠óÈüåÂ∏∂": "Anterior Cruciate Ligament[MeSH]",
    "ÂçÅÂ≠óÈüåÂ∏∂": "Anterior Cruciate Ligament[MeSH]",
    "ACL": "Anterior Cruciate Ligament[MeSH]",
    "acl": "Anterior Cruciate Ligament[MeSH]",
    "anterior cruciate ligament": "Anterior Cruciate Ligament[MeSH]",
    "ÂæåÂçÅÂ≠óÈüåÂ∏∂": "Posterior Cruciate Ligament[MeSH]",
    "PCL": "Posterior Cruciate Ligament[MeSH]",
    "pcl": "Posterior Cruciate Ligament[MeSH]",
    "ÂçäÊúàÊùø": "Menisci, Tibial[MeSH]",
    "meniscus": "Menisci, Tibial[MeSH]",
    "È´ïÈ™®": "Patella[MeSH]",
    "patella": "Patella[MeSH]",
    "È´ÇËÑõÊùü": "Iliotibial Band Syndrome[MeSH]",
    # Shoulder
    "ËÇ©ÈóúÁØÄ": "Shoulder Joint[MeSH]",
    "ÊóãËΩâËÇå": "Rotator Cuff[MeSH]",
    "rotator cuff": "Rotator Cuff[MeSH]",
    "ËÇ©Â§æÊì†": "Shoulder Impingement Syndrome[MeSH]",
    # Spine
    "ËÖ∞Áóõ": "Low Back Pain[MeSH]",
    "low back pain": "Low Back Pain[MeSH]",
    "È†∏Ê§é": "Cervical Vertebrae[MeSH]",
    "ËÖ∞Ê§é": "Lumbar Vertebrae[MeSH]",
    "Ê§éÈñìÁõ§": "Intervertebral Disc[MeSH]",
    # Neuro
    "Âπ≥Ë°°": "Postural Balance[MeSH]",
    "balance": "Postural Balance[MeSH]",
    "Ê≠•ÊÖã": "Gait[MeSH]",
    "gait": "Gait[MeSH]",
    "Êú¨È´îÊÑüË¶∫": "Proprioception[MeSH]",
    "proprioception": "Proprioception[MeSH]",
    "ËÇåÂäõ": "Muscle Strength[MeSH]",
    "muscle strength": "Muscle Strength[MeSH]",
}


class AcademicSearchSkill(BaseSkill):
    name = "academic_search"
    description = "Â≠∏Ë°ìË´ñÊñáÊêúÂ∞ã ‚Äî PubMed„ÄÅSemantic Scholar„ÄÅOpenAlexÔºàÂÖçË≤ªÔºåÁâ©ÁêÜÊ≤ªÁôÇÂ∞àÁî®Ôºâ"
    triggers = [
        "Ë´ñÊñá", "paper", "ÊúüÂàä", "journal", "pubmed", "Á†îÁ©∂", "ÊñáÁçª",
        "Â≠∏Ë°ì", "academic", "physical therapy", "Áâ©ÁêÜÊ≤ªÁôÇ", "ÊñáÁçªÊêúÂ∞ã",
        "semantic scholar", "openalex", "ÂâçÂçÅÂ≠óÈüåÂ∏∂", "ACL", "acl",
        "ÂçäÊúàÊùø", "meniscus", "ÊóãËΩâËÇå", "rotator cuff", "Ê§éÈñìÁõ§",
        "ÊâæÊñáÁçª", "ÊâæË´ñÊñá", "Êü•Ë´ñÊñá", "Êü•ÊñáÁçª", "ÊêúË´ñÊñá",
    ]
    category = "academic"
    requires_llm = False

    instructions = (
        "Â≠∏Ë°ìÊêúÂ∞ãÔºö\n"
        "1. PubMedÔºö„ÄåË´ñÊñá physical therapy stroke„Äç\n"
        "2. Semantic ScholarÔºö„ÄåË´ñÊñá semantic scholar knee rehabilitation„Äç\n"
        "3. Ëá™ÂãïÂ¢ûÂº∑ PT Áõ∏Èóú MeSH Ë°ìË™û\n"
        "4. ÊêúÂ∞ãÂæåÂèØË™™„ÄåÂ≠òÂà∞È™®ÁßëÁ≠ÜË®ò„ÄçÂÑ≤Â≠òÁµêÊûú"
    )

    intent_patterns = [
        r"(Êâæ|Êü•|Êêú).{0,5}(Áõ∏Èóú|ÊúâÈóú|ÈóúÊñº).{0,15}(Ë´ñÊñá|Á†îÁ©∂|ÊñáÁçª|ÊúüÂàä)",
        r"(ÊúâÊ≤íÊúâ|Êúâ‰ªÄÈ∫º).{0,10}(Á†îÁ©∂|Ë´ñÊñá|ÊñáÁçª).{0,10}(ÈóúÊñº|ÊúâÈóú|ÈáùÂ∞ç)",
        r"(PubMed|pubmed|Semantic Scholar).{0,20}",
        r"(ACL|PCL|ÂçäÊúàÊùø|ÊóãËΩâËÇå|Ê§éÈñìÁõ§|ÂâçÂçÅÂ≠óÈüåÂ∏∂|ÂæåÂçÅÂ≠óÈüåÂ∏∂).{0,20}(Á†îÁ©∂|Ë´ñÊñá|ÊñáÁçª|Âæ©ÂÅ•|Ê≤ªÁôÇ)",
        r"(Áâ©ÁêÜÊ≤ªÁôÇ|Âæ©ÂÅ•|PT).{0,10}(Á†îÁ©∂|Ë´ñÊñá|ÂØ¶Ë≠â|evidence)",
    ]

    # Only these trigger words get stripped from the query (not medical terms)
    _STRIP_TRIGGERS = [
        "Ë´ñÊñá", "paper", "ÊúüÂàä", "journal", "pubmed", "ÊñáÁçª", "Â≠∏Ë°ì", "academic",
        "physical therapy", "Áâ©ÁêÜÊ≤ªÁôÇ", "ÊñáÁçªÊêúÂ∞ã", "semantic scholar", "openalex",
        "ÊâæÊñáÁçª", "ÊâæË´ñÊñá", "Êü•Ë´ñÊñá", "Êü•ÊñáÁçª", "ÊêúË´ñÊñá",
    ]

    # Filler words to strip from query before searching
    _FILLER = ["Êü•ÊúâÈóú", "Êü•‰∏Ä‰∏ã", "Êâæ‰∏Ä‰∏ã", "Âπ´ÊàëÊâæ", "Âπ´ÊàëÊü•", "Áõ∏ÈóúÁöÑ", "Áõ∏Èóú",
               "ÊúâÂì™‰∫õ", "ÊúâÊ≤íÊúâ", "ÁöÑË´ñÊñá", "ÁöÑÊúüÂàä", "ÁöÑÁ†îÁ©∂", "ÁöÑÊñáÁçª",
               "Êü•Ë©¢", "ÊêúÂ∞ã", "ÊêúÁ¥¢", "Êü•Êâæ", "Ë≥áÊñô"]

    # Save-to-notes action keywords
    _SAVE_TRIGGERS = ["Â≠òÂà∞", "Â≠òÈÄ≤", "ÂÑ≤Â≠ò", "Ë®òÈåÑ", "Âä†Âà∞", "Âä†ÂÖ•", "save to", "save"]

    # Class-level cache: last search results per session
    _last_results: dict[str, list[dict]] = {}

    async def execute(self, query: str, context: dict[str, Any]) -> SkillResult:
        session_id = context.get("session_id", "default")
        raw_query = query

        # Check if this is a "save to notes" action
        if any(kw in raw_query for kw in self._SAVE_TRIGGERS) and any(
            kw in raw_query for kw in ["Á≠ÜË®ò", "note", "È™®Áßë", "notes"]
        ):
            return await self._save_to_notes(raw_query, session_id, context)

        # Clean query ‚Äî remove routing triggers and filler words (medical terms kept)
        for t in self._STRIP_TRIGGERS:
            query = re.sub(re.escape(t), " ", query, flags=re.IGNORECASE)
        for f in self._FILLER:
            query = query.replace(f, " ")
        query = re.sub(r"\s+", " ", query).strip(" ?ÔºüÔºå,„ÄÇ.„ÄÅ")

        if not query or len(query) < 2:
            return SkillResult(
                content="Ë´ãÊèê‰æõÊêúÂ∞ãÈóúÈçµÂ≠óÔºå‰æãÂ¶ÇÔºö„ÄåË´ñÊñá ÂâçÂçÅÂ≠óÈüåÂ∏∂ Âæ©ÂÅ•„Äç",
                success=False, source=self.name,
            )

        text_lower = query.lower()

        # Decide which database to search
        if "semantic scholar" in text_lower or "s2" in text_lower:
            query = query.replace("semantic scholar", "").replace("s2", "").strip()
            result = await self._search_semantic_scholar(query, session_id)
        elif "openalex" in text_lower:
            query = query.replace("openalex", "").strip()
            result = await self._search_openalex(query, session_id)
        else:
            result = await self._search_pubmed(query, session_id)

        # Append save hint and return
        if result.success:
            result.content += "\n\nüí° Ëº∏ÂÖ•„ÄåÂ≠òÂà∞È™®ÁßëÁ≠ÜË®ò„ÄçÂèØÂ∞á‰ª•‰∏äË´ñÊñáÂÑ≤Â≠òËá≥Á≠ÜË®òÁ≥ªÁµ±„ÄÇ"
            # Store metadata for potential save action
            result.metadata["query"] = query
            result.metadata["session_id"] = session_id
        return result

    async def _save_to_notes(self, query: str, session_id: str, context: dict[str, Any]) -> SkillResult:
        """Save last search results to study_notes DB."""
        import sqlite3, time
        from nexus import config

        cached = self._last_results.get(session_id, [])
        if not cached:
            return SkillResult(
                content="Êâæ‰∏çÂà∞ÂèØÂÑ≤Â≠òÁöÑË´ñÊñá„ÄÇË´ãÂÖàÊêúÂ∞ãË´ñÊñáÔºåÂÜçË™™„ÄåÂ≠òÂà∞È™®ÁßëÁ≠ÜË®ò„Äç„ÄÇ",
                success=False, source=self.name,
            )

        # Detect subject from query
        subject = "orthopedics"
        subject_map = {
            "È™®Áßë": "orthopedics", "Âæ©ÂÅ•": "rehabilitation", "Á•ûÁ∂ì": "neurology",
            "ÂøÉËÇ∫": "cardiopulmonary", "Â∞èÂÖí": "pediatrics", "ËÄÅ‰∫∫": "geriatrics",
        }
        for kw, subj in subject_map.items():
            if kw in query:
                subject = subj
                break

        db_path = config.data_dir() / "study_notes.db"
        try:
            conn = sqlite3.connect(str(db_path))
            conn.execute("""CREATE TABLE IF NOT EXISTS notes (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                subject TEXT NOT NULL, chapter TEXT DEFAULT '',
                content TEXT NOT NULL, tags TEXT DEFAULT '',
                timestamp REAL NOT NULL, date TEXT NOT NULL)""")
            now = time.time()
            date_str = time.strftime("%Y-%m-%d")
            saved = 0
            for paper in cached:
                content = f"[Ë´ñÊñá] {paper.get('title', '')} | {paper.get('authors', '')} | {paper.get('journal', '')} {paper.get('year', '')} | {paper.get('url', paper.get('pmid', ''))}"
                conn.execute(
                    "INSERT INTO notes (subject, content, tags, timestamp, date) VALUES (?, ?, ?, ?, ?)",
                    (subject, content[:500], "Ë´ñÊñá,academic_search", now + saved * 0.001, date_str),
                )
                saved += 1
            conn.commit()
            conn.close()

            subject_zh = {"orthopedics": "È™®Áßë", "rehabilitation": "Âæ©ÂÅ•"}.get(subject, subject)
            return SkillResult(
                content=f"üìö Â∑≤Â∞á **{saved} ÁØáË´ñÊñá**ÂÑ≤Â≠òËá≥„Äå{subject_zh}„ÄçÁ≠ÜË®òÔºÅ\nËº∏ÂÖ•„ÄåÁ≠ÜË®ò Ë§áÁøí È™®Áßë„ÄçÂèØÊü•Áúã„ÄÇ",
                success=True, source=self.name,
            )
        except Exception as e:
            return SkillResult(content=f"ÂÑ≤Â≠òÂ§±ÊïóÔºö{e}", success=False, source=self.name)

    async def _search_pubmed(self, query: str, session_id: str = "default") -> SkillResult:
        """Search PubMed via E-utilities API (free, 3 req/sec)."""
        import httpx

        # Auto-enhance with MeSH terms
        enhanced_query = self._enhance_query(query)

        try:
            async with httpx.AsyncClient(timeout=15) as client:
                # Step 1: Search for PMIDs
                search_resp = await client.get(
                    "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
                    params={
                        "db": "pubmed",
                        "term": enhanced_query,
                        "retmode": "json",
                        "retmax": 8,
                        "sort": "relevance",
                    },
                )
                search_data = search_resp.json()
                pmids = search_data.get("esearchresult", {}).get("idlist", [])
                total = search_data.get("esearchresult", {}).get("count", "0")

                if not pmids:
                    return SkillResult(
                        content=f"PubMed ÊêúÂ∞ã„Äå{query}„ÄçÊ≤íÊúâÊâæÂà∞ÁµêÊûú„ÄÇ\nÊêúÂ∞ãË™ûÊ≥ï: {enhanced_query}",
                        success=True, source=self.name,
                    )

                # Step 2: Fetch article details
                fetch_resp = await client.get(
                    "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi",
                    params={
                        "db": "pubmed",
                        "id": ",".join(pmids),
                        "retmode": "xml",
                        "rettype": "abstract",
                    },
                )

                articles = self._parse_pubmed_xml(fetch_resp.text)

                # Cache for save-to-notes action
                cache_items = []
                lines = [f"üìö **PubMed ÊêúÂ∞ãÁµêÊûú**ÔºàÂÖ± {total} Á≠ÜÔºåÈ°ØÁ§∫ {len(articles)} Á≠ÜÔºâ\n"]
                for i, article in enumerate(articles, 1):
                    lines.append(f"**{i}. {article['title']}**")
                    if article.get("authors"):
                        lines.append(f"   üë§ {article['authors']}")
                    if article.get("journal"):
                        lines.append(f"   üìñ {article['journal']} ({article.get('year', '')})")
                    if article.get("pmid"):
                        lines.append(f"   üîó https://pubmed.ncbi.nlm.nih.gov/{article['pmid']}/")
                    if article.get("abstract"):
                        lines.append(f"   üìù {article['abstract'][:150]}...")
                    lines.append("")
                    cache_items.append({
                        "title": article.get("title", ""),
                        "authors": article.get("authors", ""),
                        "journal": article.get("journal", ""),
                        "year": article.get("year", ""),
                        "pmid": article.get("pmid", ""),
                        "url": f"https://pubmed.ncbi.nlm.nih.gov/{article.get('pmid', '')}/",
                    })

                # Save to class cache
                AcademicSearchSkill._last_results[session_id] = cache_items

                return SkillResult(content="\n".join(lines), success=True, source=self.name)

        except Exception as e:
            return SkillResult(content=f"PubMed ÊêúÂ∞ãÂ§±Êïó: {e}", success=False, source=self.name)

    async def _search_semantic_scholar(self, query: str, session_id: str = "default") -> SkillResult:
        """Search Semantic Scholar API (free, no key needed)."""
        import httpx

        try:
            async with httpx.AsyncClient(timeout=15) as client:
                resp = await client.get(
                    "https://api.semanticscholar.org/graph/v1/paper/search",
                    params={
                        "query": query,
                        "limit": 8,
                        "fields": "title,authors,year,abstract,url,citationCount,openAccessPdf",
                    },
                )
                resp.raise_for_status()
                data = resp.json()

            papers = data.get("data", [])
            total = data.get("total", 0)

            if not papers:
                return SkillResult(
                    content=f"Semantic Scholar ÊêúÂ∞ã„Äå{query}„ÄçÊ≤íÊúâÊâæÂà∞ÁµêÊûú„ÄÇ",
                    success=True, source=self.name,
                )

            lines = [f"üìö **Semantic Scholar ÊêúÂ∞ãÁµêÊûú**ÔºàÂÖ± {total:,} Á≠ÜÔºâ\n"]
            cache_items = []
            for i, paper in enumerate(papers, 1):
                title = paper.get("title", "Untitled")
                year = paper.get("year", "")
                citations = paper.get("citationCount", 0)
                authors = ", ".join(a.get("name", "") for a in paper.get("authors", [])[:3])
                pdf = paper.get("openAccessPdf", {})
                pdf_url = pdf.get("url", "") if pdf else ""

                lines.append(f"**{i}. {title}**")
                if authors:
                    lines.append(f"   üë§ {authors}")
                lines.append(f"   üìÖ {year} | üìä ÂºïÁî®: {citations}")
                if paper.get("url"):
                    lines.append(f"   üîó {paper['url']}")
                if pdf_url:
                    lines.append(f"   üìÑ PDF: {pdf_url}")
                lines.append("")
                cache_items.append({
                    "title": title, "authors": authors,
                    "year": str(year), "url": paper.get("url", ""),
                })

            AcademicSearchSkill._last_results[session_id] = cache_items
            return SkillResult(content="\n".join(lines), success=True, source=self.name)

        except Exception as e:
            return SkillResult(content=f"Semantic Scholar ÊêúÂ∞ãÂ§±Êïó: {e}", success=False, source=self.name)

    async def _search_openalex(self, query: str, session_id: str = "default") -> SkillResult:
        """Search OpenAlex API (free, massive coverage)."""
        import httpx

        try:
            async with httpx.AsyncClient(timeout=15) as client:
                resp = await client.get(
                    "https://api.openalex.org/works",
                    params={"search": query, "per_page": 8},
                )
                resp.raise_for_status()
                data = resp.json()

            works = data.get("results", [])
            total = data.get("meta", {}).get("count", 0)

            if not works:
                return SkillResult(
                    content=f"OpenAlex ÊêúÂ∞ã„Äå{query}„ÄçÊ≤íÊúâÊâæÂà∞ÁµêÊûú„ÄÇ",
                    success=True, source=self.name,
                )

            lines = [f"üìö **OpenAlex ÊêúÂ∞ãÁµêÊûú**ÔºàÂÖ± {total:,} Á≠ÜÔºâ\n"]
            cache_items = []
            for i, work in enumerate(works, 1):
                title = work.get("title", "Untitled")
                year = work.get("publication_year", "")
                cited = work.get("cited_by_count", 0)
                doi = work.get("doi", "")
                oa = work.get("open_access", {})
                oa_url = oa.get("oa_url", "") if oa else ""
                is_oa = oa.get("is_oa", False) if oa else False

                lines.append(f"**{i}. {title}**")
                lines.append(f"   üìÖ {year} | üìä ÂºïÁî®: {cited}" + (" | üîì Open Access" if is_oa else ""))
                if doi:
                    lines.append(f"   üîó {doi}")
                if oa_url:
                    lines.append(f"   üìÑ PDF: {oa_url}")
                lines.append("")
                cache_items.append({
                    "title": title, "year": str(year), "url": doi or oa_url,
                })

            AcademicSearchSkill._last_results[session_id] = cache_items
            return SkillResult(content="\n".join(lines), success=True, source=self.name)

        except Exception as e:
            return SkillResult(content=f"OpenAlex ÊêúÂ∞ãÂ§±Êïó: {e}", success=False, source=self.name)

    def _enhance_query(self, query: str) -> str:
        """Auto-enhance query with MeSH terms for PT-related searches."""
        query_lower = query.lower()
        for keyword, mesh in PT_MESH_TERMS.items():
            if keyword in query_lower:
                # Replace keyword with MeSH term for better PubMed results
                query = query_lower.replace(keyword, mesh, 1)
                return query
        return query

    def _parse_pubmed_xml(self, xml_text: str) -> list[dict[str, str]]:
        """Parse PubMed XML response into article dicts."""
        articles = []
        try:
            root = ET.fromstring(xml_text)
            for article_el in root.findall(".//PubmedArticle"):
                article = {}

                # PMID
                pmid_el = article_el.find(".//PMID")
                if pmid_el is not None:
                    article["pmid"] = pmid_el.text

                # Title
                title_el = article_el.find(".//ArticleTitle")
                if title_el is not None:
                    article["title"] = "".join(title_el.itertext()).strip()

                # Authors
                authors = []
                for author_el in article_el.findall(".//Author")[:3]:
                    last = author_el.findtext("LastName", "")
                    init = author_el.findtext("Initials", "")
                    if last:
                        authors.append(f"{last} {init}".strip())
                article["authors"] = ", ".join(authors) + (" et al." if len(article_el.findall(".//Author")) > 3 else "")

                # Journal
                journal_el = article_el.find(".//Journal/Title")
                if journal_el is not None:
                    article["journal"] = journal_el.text

                # Year
                year_el = article_el.find(".//PubDate/Year")
                if year_el is not None:
                    article["year"] = year_el.text

                # Abstract
                abstract_parts = []
                for abs_el in article_el.findall(".//AbstractText"):
                    text = "".join(abs_el.itertext()).strip()
                    if text:
                        abstract_parts.append(text)
                article["abstract"] = " ".join(abstract_parts)

                if article.get("title"):
                    articles.append(article)
        except ET.ParseError:
            pass
        return articles
