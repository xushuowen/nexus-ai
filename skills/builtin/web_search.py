"""Web search skill using DuckDuckGo (free, no API key needed)."""

from __future__ import annotations

import re
from typing import Any

from nexus.skills.skill_base import BaseSkill, SkillResult


class WebSearchSkill(BaseSkill):
    name = "web_search"
    description = "ç¶²è·¯æœå°‹ â€” ä½¿ç”¨ DuckDuckGoï¼ˆå…è²»ï¼Œä¸éœ€è¦ API keyï¼‰"
    triggers = ["æœå°‹", "search", "æŸ¥ä¸€ä¸‹", "google", "æ‰¾ä¸€ä¸‹", "look up", "æŸ¥è©¢", "å¹«æˆ‘æ‰¾", "ç¶²è·¯æŸ¥"]
    intent_patterns = [
        r"(æˆ‘æƒ³çŸ¥é“|æƒ³äº†è§£|å¹«æˆ‘æŸ¥|æƒ³æŸ¥).{2,30}",
        r"(æœ‰æ²’æœ‰|æœ‰é—œ|é—œæ–¼).{2,20}(è³‡æ–™|è³‡è¨Š|æ¶ˆæ¯|ä»‹ç´¹)",
        r"(æœ€æ–°|ç¾åœ¨).{0,5}(æœ‰ä»€éº¼|ç™¼ç”Ÿäº†|æ€éº¼äº†).{0,15}",
        r"(ä»€éº¼æ˜¯|èª°æ˜¯|å“ªè£¡æœ‰|æ€éº¼).{2,25}",
        r"(å¹«æˆ‘|è«‹).{0,5}(æ‰¾|æŸ¥|æœ).{2,20}",
    ]
    category = "web"
    requires_llm = False

    instructions = "ä½¿ç”¨ DuckDuckGo HTML æœå°‹ï¼Œæå–å‰å¹¾ç­†çµæœã€‚"

    async def execute(self, query: str, context: dict[str, Any]) -> SkillResult:
        # Strip trigger words
        for t in self.triggers:
            query = query.replace(t, "").strip()
        query = query.strip()

        if not query:
            return SkillResult(content="è«‹æä¾›æœå°‹é—œéµå­—ã€‚", success=False, source=self.name)

        try:
            import httpx
            from nexus.security.url_filter import is_url_safe

            url = f"https://html.duckduckgo.com/html/?q={query}"
            safe, reason = is_url_safe(url)
            if not safe:
                return SkillResult(content=f"URL blocked: {reason}", success=False, source=self.name)

            async with httpx.AsyncClient(timeout=10, follow_redirects=True) as client:
                resp = await client.get(url, headers={"User-Agent": "Mozilla/5.0"})
                resp.raise_for_status()
                html = resp.text

            # Parse results
            results = self._parse_results(html)
            if not results:
                return SkillResult(content=f"æœå°‹ã€Œ{query}ã€æ²’æœ‰æ‰¾åˆ°çµæœã€‚", success=True, source=self.name)

            lines = [f"ğŸ” æœå°‹ã€Œ{query}ã€æ‰¾åˆ° {len(results)} ç­†çµæœï¼š\n"]
            for i, (title, snippet, link) in enumerate(results[:5], 1):
                lines.append(f"**{i}. {title}**")
                if link:
                    lines.append(f"   ğŸ”— {link}")
                if snippet:
                    lines.append(f"   {snippet}")
                lines.append("")

            return SkillResult(content="\n".join(lines), success=True, source=self.name)

        except Exception as e:
            return SkillResult(content=f"æœå°‹å¤±æ•—: {e}", success=False, source=self.name)

    def _parse_results(self, html: str) -> list[tuple[str, str, str]]:
        """Parse DuckDuckGo HTML results. Returns (title, snippet, url)."""
        results = []
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html, "html.parser")
            for result in soup.select(".result")[:8]:
                title_el = result.select_one(".result__a")
                snippet_el = result.select_one(".result__snippet")
                url_el = result.select_one(".result__url")
                if title_el:
                    title = title_el.get_text(strip=True)
                    snippet = snippet_el.get_text(strip=True) if snippet_el else ""
                    link = url_el.get_text(strip=True) if url_el else ""
                    if link and not link.startswith("http"):
                        link = "https://" + link
                    results.append((title, snippet, link))
        except ImportError:
            # Fallback: basic regex extraction
            titles = re.findall(r'class="result__a"[^>]*>(.*?)</a>', html)
            snippets = re.findall(r'class="result__snippet"[^>]*>(.*?)</[^>]+>', html)
            urls = re.findall(r'class="result__url"[^>]*>(.*?)</[^>]+>', html)
            for i, title in enumerate(titles[:8]):
                title = re.sub(r'<[^>]+>', '', title).strip()
                snippet = re.sub(r'<[^>]+>', '', snippets[i]).strip() if i < len(snippets) else ""
                link = re.sub(r'<[^>]+>', '', urls[i]).strip() if i < len(urls) else ""
                if link and not link.startswith("http"):
                    link = "https://" + link
                results.append((title, snippet, link))
        return results
